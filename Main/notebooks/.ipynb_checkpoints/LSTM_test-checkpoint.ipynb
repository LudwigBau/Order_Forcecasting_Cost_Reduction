{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4c72251e-b6f0-4db5-a9de-308f3f00a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libaries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Flatten\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "#from keras.callbacks import EarlyStopping\n",
    "from keras.layers import ConvLSTM2D\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ludwigbaunach/Documents/Studium/PhD/Alaiko/Paper_1_Project/Main/src')\n",
    "from utils.data_split import ml_data_date_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ddc6862f-c378-42c3-92a7-67caf9213a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/processed/L_6_test.pkl\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ca90b98-f70c-4867-a490-027629c5381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(42)\n",
    "# Horizon \n",
    "time_horizon = 9\n",
    "# LSTM\n",
    "n_steps_in = 30\n",
    "n_steps_out = time_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3d071da-19d1-463a-bcd6-5b7ef2d17858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f65ca-f8a0-4bf4-8fe6-3133a0b355f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cusstom function \n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# LSTM Forecast \n",
    "def LSTM_forecast(train, test, model, drop_columns):\n",
    "\n",
    "    # drop the first two columns (assuming they're date and target variable) and get np array\n",
    "    features = train.drop(train.columns[drop_columns], axis=1).values\n",
    "    test_features = train.iloc[-n_steps_in:].drop(train.columns[drop_columns], axis=1).values  \n",
    "    \n",
    "    # reshape the target variable\n",
    "    out_seq = np.array(train.quantity).reshape((len(train), 1))\n",
    "\n",
    "    # horizontally stack features and target variable\n",
    "    dataset = np.hstack((features, out_seq))\n",
    "\n",
    "    # choose a number of time steps\n",
    "    n_steps_in, n_steps_out = 30, 9\n",
    "\n",
    "    # covert into input/output\n",
    "    X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "\n",
    "    # the dataset knows the number of features, e.g. 2\n",
    "    n_features = X.shape[2]\n",
    "    \n",
    "    # fit model\n",
    "    model.fit(X, y, epochs=200, verbose=0)\n",
    "    \n",
    "    # forecast\n",
    "    \n",
    "    # reshape test_features\n",
    "    x_test_input = test_features.reshape((1, n_steps_in, n_features))\n",
    "    #predict\n",
    "    yhat = model.predict(x_test_input, verbose=0)\n",
    "    # make yhat 1-dimensional\n",
    "    yhat_1d = yhat.flatten()\n",
    "\n",
    "    return yhat_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566add7-a311-4514-9ff0-d4195bfeda84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "38c9c065-e56a-44ed-9c38-489b507867af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff11af7c970>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "    model.add(LSTM(100, activation='relu'))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4e837c5d-f170-4092-bbe1-1393b29dee69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 54)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = train.iloc[-n_steps_in:].drop(train.columns[[0, 1]], axis=1).values\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "eeb82e9e-9a22-4466-929c-54a449d4e91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.350144006640273"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all features from the third column onward, for the last 30 entries\n",
    "test_features = train.iloc[-n_steps_in:].drop(train.columns[[0, 1]], axis=1).values\n",
    "\n",
    "# reshape it to fit the LSTM input shape\n",
    "x_test_input = test_features.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_test_input, verbose=0)\n",
    "yhat_1d = yhat.flatten()\n",
    "mape(test.quantity, yhat_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c7b7bf-1787-41fa-a2ac-3fe661cd9a01",
   "metadata": {},
   "source": [
    "## 1. Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b026d-32aa-48a7-b6fe-860fa455c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                   activation='relu', return_sequences=True, \n",
    "                   input_shape=(n_steps_in, n_features)))\n",
    "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "                   activation='relu'))\n",
    "    model.add(Dense(n_steps_out))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# drop the first two columns (assuming they're date and target variable) and get np array\n",
    "features = train.drop(train.columns[drop_columns], axis=1).values\n",
    "test_features = train.iloc[-n_steps_in:].drop(train.columns[drop_columns], axis=1).values  \n",
    "\n",
    "# reshape the target variable\n",
    "out_seq = np.array(train.quantity).reshape((len(train), 1))\n",
    "\n",
    "# horizontally stack features and target variable\n",
    "dataset = np.hstack((features, out_seq))\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 30, 9\n",
    "\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# Keras Tuner's hyperparameter tuning\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='loss',\n",
    "    max_trials=5,  # how many model variations to test\n",
    "    executions_per_trial=3,  # how many trials per variation\n",
    "    directory='random_search',\n",
    "    project_name='lstm_forecasting'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X, y, epochs=20, validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the LSTM layer is {best_hps.get('units')}\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# forecast\n",
    "\n",
    "# reshape test_features\n",
    "x_test_input = test_features.reshape((1, n_steps_in, n_features))\n",
    "#predict\n",
    "yhat = model.predict(x_test_input, verbose=0)\n",
    "# make yhat 1-dimensional\n",
    "yhat_1d = yhat.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fdc7b-1180-40d3-b570-8002809fa595",
   "metadata": {},
   "source": [
    "## 2. Backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a889d8b-37cd-41cc-845e-03579a64b0cc",
   "metadata": {},
   "source": [
    "## 3. Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b05b27-0ec1-4b2a-b75c-e715087da951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf2306-2769-4d9c-96c3-d4209b09053e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a582243c-992f-450c-bbdf-9754c36f43e0",
   "metadata": {},
   "source": [
    "## STRATEGY\n",
    "\n",
    "1. Search for hyperparameters in training data\n",
    "2. Take best parameters and perform backtesting \n",
    "3. Forecast using the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5795e6-b305-4944-8f59-0dcb762536dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8140a72-cbc4-4db0-9a33-cb155e00e930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0348a7c7-8428-4df9-85be-ff74ea94b53a",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a76e3-df92-4a86-b8eb-b0bb5e22ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# Define LSTM model structure\n",
    "def create_lstm_model(n_steps, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', run_eagerly=True)\n",
    "    return model\n",
    "\n",
    "# Function to reshape data for LSTM\n",
    "def reshape_data(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:(i + time_steps), 1:])  # Use all but the first column as features\n",
    "        y.append(data[i + time_steps, 0])  # Use the first column as target\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_steps = 9\n",
    "results = {}\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=None, \n",
    "                                   n_splits=5, \n",
    "                                   test_size=time_horizon)\n",
    "\n",
    "\n",
    "# Define Data\n",
    "train_df, test_df = ml_data_date_split(df, 8) # split data with custom function\n",
    "# Sort values (important for time series split)\n",
    "train_sorted_df = train_df.sort_values(by=['date'])\n",
    "test_sorted_df = test_df.sort_values(by=['date'])\n",
    "\n",
    "train_sorted_df = train_sorted_df.set_index(\"date\")\n",
    "test_sorted_df = test_sorted_df.set_index(\"date\")\n",
    "\n",
    "data = train_sorted_df.values\n",
    "\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    train_data, test_data = data[train_index], data[test_index]\n",
    "\n",
    "    # Reshape data for LSTM\n",
    "    X_train, y_train = reshape_data(train_data, time_steps)\n",
    "    X_test, y_test = reshape_data(test_data, time_steps)\n",
    "\n",
    "    # Initialize and fit LSTM model\n",
    "    model = create_lstm_model(time_steps, X_train.shape[2])\n",
    "    model.fit(X_train, y_train, epochs=30, verbose=0)\n",
    "\n",
    "    # Backtest on train data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    backtest_df = pd.DataFrame({\n",
    "        \"date\": train_sorted_df.index[time_steps:len(y_train_pred)+time_steps],\n",
    "        \"actual\": y_train,\n",
    "        \"pred\": y_train_pred.flatten()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "49f97a4b-54f9-4894-8ee0-7975fb6a9b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917, 9, 54)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1650a2f8-ca34-4225-ae25-1ebeac4b3c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>actual</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>133.0</td>\n",
       "      <td>59.722706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>99.0</td>\n",
       "      <td>65.757957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>102.0</td>\n",
       "      <td>107.461853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-04-12</td>\n",
       "      <td>100.0</td>\n",
       "      <td>188.496429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-04-13</td>\n",
       "      <td>90.0</td>\n",
       "      <td>187.498001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2022-11-13</td>\n",
       "      <td>31567.0</td>\n",
       "      <td>23244.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>16820.0</td>\n",
       "      <td>16320.597656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>13479.0</td>\n",
       "      <td>12720.399414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>16334.0</td>\n",
       "      <td>13939.563477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>19117.0</td>\n",
       "      <td>12533.387695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>953 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date   actual          pred\n",
       "0   2020-04-09    133.0     59.722706\n",
       "1   2020-04-10     99.0     65.757957\n",
       "2   2020-04-11    102.0    107.461853\n",
       "3   2020-04-12    100.0    188.496429\n",
       "4   2020-04-13     90.0    187.498001\n",
       "..         ...      ...           ...\n",
       "948 2022-11-13  31567.0  23244.406250\n",
       "949 2022-11-14  16820.0  16320.597656\n",
       "950 2022-11-15  13479.0  12720.399414\n",
       "951 2022-11-16  16334.0  13939.563477\n",
       "952 2022-11-17  19117.0  12533.387695\n",
       "\n",
       "[953 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99069036-c632-4993-aa86-4738c676bbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 55)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b78f6b83-02df-4e58-8252-a5387971f46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 - 4s - loss: 2947.3850 - val_loss: 9954.5234 - 4s/epoch - 715ms/step\n",
      "Epoch 2/50\n",
      "6/6 - 0s - loss: 2947.3096 - val_loss: 9954.4443 - 89ms/epoch - 15ms/step\n",
      "Epoch 3/50\n",
      "6/6 - 0s - loss: 2947.1707 - val_loss: 9954.2998 - 85ms/epoch - 14ms/step\n",
      "Epoch 4/50\n",
      "6/6 - 0s - loss: 2947.0928 - val_loss: 9954.2471 - 84ms/epoch - 14ms/step\n",
      "Epoch 5/50\n",
      "6/6 - 0s - loss: 2947.0278 - val_loss: 9954.1934 - 92ms/epoch - 15ms/step\n",
      "Epoch 6/50\n",
      "6/6 - 0s - loss: 2946.9619 - val_loss: 9954.1387 - 71ms/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "6/6 - 0s - loss: 2946.8948 - val_loss: 9954.0850 - 70ms/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "6/6 - 0s - loss: 2946.8254 - val_loss: 9954.0322 - 77ms/epoch - 13ms/step\n",
      "Epoch 9/50\n",
      "6/6 - 0s - loss: 2946.7527 - val_loss: 9953.9775 - 70ms/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "6/6 - 0s - loss: 2946.6724 - val_loss: 9953.8857 - 72ms/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "6/6 - 0s - loss: 2946.4285 - val_loss: 9953.6338 - 70ms/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "6/6 - 0s - loss: 2946.2908 - val_loss: 9953.3818 - 77ms/epoch - 13ms/step\n",
      "Epoch 13/50\n",
      "6/6 - 0s - loss: 2945.9385 - val_loss: 9953.0215 - 79ms/epoch - 13ms/step\n",
      "Epoch 14/50\n",
      "6/6 - 0s - loss: 2945.6921 - val_loss: 9952.8643 - 85ms/epoch - 14ms/step\n",
      "Epoch 15/50\n",
      "6/6 - 0s - loss: 2945.4414 - val_loss: 9952.5029 - 77ms/epoch - 13ms/step\n",
      "Epoch 16/50\n",
      "6/6 - 0s - loss: 2944.7949 - val_loss: 9951.6621 - 85ms/epoch - 14ms/step\n",
      "Epoch 17/50\n",
      "6/6 - 0s - loss: 2944.4568 - val_loss: 9951.5488 - 80ms/epoch - 13ms/step\n",
      "Epoch 18/50\n",
      "6/6 - 0s - loss: 2944.3105 - val_loss: 9951.4297 - 79ms/epoch - 13ms/step\n",
      "Epoch 19/50\n",
      "6/6 - 0s - loss: 2944.1567 - val_loss: 9951.3125 - 79ms/epoch - 13ms/step\n",
      "Epoch 20/50\n",
      "6/6 - 0s - loss: 2943.9722 - val_loss: 9950.9932 - 82ms/epoch - 14ms/step\n",
      "Epoch 21/50\n",
      "6/6 - 0s - loss: 2943.6091 - val_loss: 9950.6514 - 71ms/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "6/6 - 0s - loss: 2943.2798 - val_loss: 9950.4941 - 64ms/epoch - 11ms/step\n",
      "Epoch 23/50\n",
      "6/6 - 0s - loss: 2942.9524 - val_loss: 9949.8604 - 77ms/epoch - 13ms/step\n",
      "Epoch 24/50\n",
      "6/6 - 0s - loss: 2942.3135 - val_loss: 9949.3135 - 75ms/epoch - 13ms/step\n",
      "Epoch 25/50\n",
      "6/6 - 0s - loss: 2941.6445 - val_loss: 9948.5957 - 74ms/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "6/6 - 0s - loss: 2941.2434 - val_loss: 9948.3594 - 105ms/epoch - 18ms/step\n",
      "Epoch 27/50\n",
      "6/6 - 0s - loss: 2940.9380 - val_loss: 9948.0479 - 87ms/epoch - 15ms/step\n",
      "Epoch 28/50\n",
      "6/6 - 0s - loss: 2940.6145 - val_loss: 9947.8223 - 82ms/epoch - 14ms/step\n",
      "Epoch 29/50\n",
      "6/6 - 0s - loss: 2940.1990 - val_loss: 9946.9189 - 83ms/epoch - 14ms/step\n",
      "Epoch 30/50\n",
      "6/6 - 0s - loss: 2939.1367 - val_loss: 9946.0029 - 75ms/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "6/6 - 0s - loss: 2938.4521 - val_loss: 9945.2988 - 135ms/epoch - 23ms/step\n",
      "Epoch 32/50\n",
      "6/6 - 0s - loss: 2938.1038 - val_loss: 9944.8760 - 95ms/epoch - 16ms/step\n",
      "Epoch 33/50\n",
      "6/6 - 0s - loss: 2937.7549 - val_loss: 9944.5869 - 100ms/epoch - 17ms/step\n",
      "Epoch 34/50\n",
      "6/6 - 0s - loss: 2937.4658 - val_loss: 9944.3027 - 92ms/epoch - 15ms/step\n",
      "Epoch 35/50\n",
      "6/6 - 0s - loss: 2937.1794 - val_loss: 9944.0264 - 129ms/epoch - 21ms/step\n",
      "Epoch 36/50\n",
      "6/6 - 0s - loss: 2936.8943 - val_loss: 9943.7568 - 93ms/epoch - 16ms/step\n",
      "Epoch 37/50\n",
      "6/6 - 0s - loss: 2936.5884 - val_loss: 9943.4561 - 71ms/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "6/6 - 0s - loss: 2936.2439 - val_loss: 9942.9688 - 75ms/epoch - 12ms/step\n",
      "Epoch 39/50\n",
      "6/6 - 0s - loss: 2935.8518 - val_loss: 9942.7021 - 85ms/epoch - 14ms/step\n",
      "Epoch 40/50\n",
      "6/6 - 0s - loss: 2935.5623 - val_loss: 9942.4375 - 73ms/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "6/6 - 0s - loss: 2935.2212 - val_loss: 9941.8447 - 78ms/epoch - 13ms/step\n",
      "Epoch 42/50\n",
      "6/6 - 0s - loss: 2934.7209 - val_loss: 9941.5596 - 72ms/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "6/6 - 0s - loss: 2934.4219 - val_loss: 9941.2734 - 76ms/epoch - 13ms/step\n",
      "Epoch 44/50\n",
      "6/6 - 0s - loss: 2934.0317 - val_loss: 9940.4102 - 114ms/epoch - 19ms/step\n",
      "Epoch 45/50\n",
      "6/6 - 0s - loss: 2933.3374 - val_loss: 9939.9854 - 73ms/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "6/6 - 0s - loss: 2932.9836 - val_loss: 9939.6670 - 77ms/epoch - 13ms/step\n",
      "Epoch 47/50\n",
      "6/6 - 0s - loss: 2932.6707 - val_loss: 9939.3545 - 101ms/epoch - 17ms/step\n",
      "Epoch 48/50\n",
      "6/6 - 0s - loss: 2932.3613 - val_loss: 9939.0449 - 122ms/epoch - 20ms/step\n",
      "Epoch 49/50\n",
      "6/6 - 0s - loss: 2932.0544 - val_loss: 9938.7402 - 81ms/epoch - 13ms/step\n",
      "Epoch 50/50\n",
      "6/6 - 0s - loss: 2931.7483 - val_loss: 9938.4414 - 131ms/epoch - 22ms/step\n",
      "19/19 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Frame the supervised learning problem\n",
    "n_days = 9  # number of days to forecast\n",
    "n_features = df.shape[1]  # number of features in the data\n",
    "n_obs = n_days * n_features\n",
    "reframed = series_to_supervised(df, n_days, n_days)\n",
    "values = reframed.values\n",
    "\n",
    "# Split into train and test sets\n",
    "n_train_days = 365  # number of days to use for training\n",
    "train = values[:n_train_days, :]\n",
    "test = values[n_train_days:, :]\n",
    "# Split into input and outputs\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], n_days, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_days, n_features))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss=root_mean_squared_error)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "# Make a prediction\n",
    "yhat = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc0f1d35-8f44-4469-b7ca-e97641ef9958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.599987],\n",
       "       [16.599985],\n",
       "       [16.599976],\n",
       "       [16.59997 ],\n",
       "       [16.59997 ],\n",
       "       [16.59997 ],\n",
       "       [16.59998 ],\n",
       "       [16.59998 ],\n",
       "       [16.59998 ],\n",
       "       [16.6     ],\n",
       "       [16.600025],\n",
       "       [16.600075],\n",
       "       [16.600092],\n",
       "       [16.600113],\n",
       "       [16.600136],\n",
       "       [16.60017 ],\n",
       "       [16.600563],\n",
       "       [16.600552],\n",
       "       [16.600569],\n",
       "       [16.600538],\n",
       "       [16.600536],\n",
       "       [16.600506],\n",
       "       [16.60042 ],\n",
       "       [16.600191],\n",
       "       [16.60005 ],\n",
       "       [16.600014],\n",
       "       [16.599993],\n",
       "       [16.600428],\n",
       "       [16.601051],\n",
       "       [16.601183],\n",
       "       [16.60244 ],\n",
       "       [16.610025],\n",
       "       [16.610346],\n",
       "       [16.611515],\n",
       "       [16.611193],\n",
       "       [16.610655],\n",
       "       [16.610048],\n",
       "       [16.608297],\n",
       "       [16.603857],\n",
       "       [16.601175],\n",
       "       [16.600372],\n",
       "       [16.599981],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599987],\n",
       "       [16.599987],\n",
       "       [16.599987],\n",
       "       [16.599987],\n",
       "       [16.599987],\n",
       "       [16.599987],\n",
       "       [16.599985],\n",
       "       [16.599974],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599972],\n",
       "       [16.599972],\n",
       "       [16.599972],\n",
       "       [16.599972],\n",
       "       [16.599972],\n",
       "       [16.599972],\n",
       "       [16.599972],\n",
       "       [16.59997 ],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968],\n",
       "       [16.599968]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5879986-462d-4328-9df8-5bf24e3a9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_cols = ['q_roll_mean_7d', 'q_roll_std_7d', 'q_roll_mean_14d', 'q_roll_std_14d', 'q_lag_1d',\n",
    "       'q_lag_7d', 'q_lag_14d', 'q_lag_28d', 'q_mean_lag_7_14_28',\n",
    "       'precipitation_height', 'sunshine_duration', 'temperature_air_mean_200',\n",
    "       'sunshine_duration_h', 'suns_classes', 'temp_classes', \"rain_classes\"]\n",
    "\n",
    "       \n",
    "encode_cols = [\"tm_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482fbe1c-38fa-4d31-8836-011a70be9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make year a catagory\n",
    "df[encode_cols] = df[encode_cols].astype('category',copy=False)\n",
    "\n",
    "# Get dummies\n",
    "just_dummies = pd.get_dummies(df[encode_cols])\n",
    "encoded_df = pd.concat([df, just_dummies], axis=1)      \n",
    "encoded_df.drop(encode_cols, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff339db0-5220-4b19-b67f-814a5bfd47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "# splot data\n",
    "x_train, x_test = ml_data_split(encoded_df.reset_index(), 10)\n",
    "\n",
    "# get y that has to be predicted\n",
    "y_train = x_train['quantity'].copy()\n",
    "y_test = x_test['quantity'].copy()\n",
    "\n",
    "# clean data: drop y variable and date from dataset\n",
    "x_train.drop([\"quantity\", 'date'], axis=1, inplace=True)\n",
    "x_test.drop([\"quantity\", 'date'], axis=1, inplace=True)\n",
    "\n",
    "# Scale the selected columns using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_x_train = x_train.copy()\n",
    "scaled_x_test = x_test.copy()\n",
    "\n",
    "scaled_x_train[scale_cols] = scaler.fit_transform(x_train[scale_cols])\n",
    "scaled_x_test[scale_cols] = scaler.transform(x_test[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f332ec9a-182c-4c6c-ae69-27815ed02eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all data is float\n",
    "train_values = scaled_x_train.values.astype('float32')\n",
    "test_values = scaled_x_test.values.astype('float32')\n",
    "#train_values.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f016713-18aa-478f-877e-f012d6817f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 54)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "268534d1-bee5-4eb4-a2d6-8d999cb9f447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_test:  (969, 54)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 52326 into shape (969,10,54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#shape of train_test\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of train_test: \u001b[39m\u001b[38;5;124m\"\u001b[39m,scaled_x_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m969\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m54\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 52326 into shape (969,10,54)"
     ]
    }
   ],
   "source": [
    "train_test = scaled_x_train.to_numpy()\n",
    "\n",
    "# Set up array\n",
    "\n",
    "n_future = 10\n",
    "n_past = 60\n",
    "\n",
    "#shape of train_test\n",
    "print(\"Shape of train_test: \",scaled_x_train.shape)\n",
    "\n",
    "data = train_test.reshape(969, 10, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a822313-c709-4b5b-8520-809facb953f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 14  # Number of past days we want to use to predict the future.\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1943468e-86c3-40d3-8123-275edcf911f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 60, None), slice(0, 54, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/_libs/index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(0, 60, None), slice(0, 54, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Reformat input data into a shape: (n_samples x timesteps x n_features)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#In my example, my df_for_training_scaled has a shape (12823, 5)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_past, \u001b[38;5;28mlen\u001b[39m(scaled_x_train) \u001b[38;5;241m-\u001b[39m n_future \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     trainX\u001b[38;5;241m.\u001b[39mappend(\u001b[43mscaled_x_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mscaled_x_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     13\u001b[0m     trainY\u001b[38;5;241m.\u001b[39mappend(scaled_x_train[i \u001b[38;5;241m+\u001b[39m n_future \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:i \u001b[38;5;241m+\u001b[39m n_future, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m trainX, trainY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(trainX), np\u001b[38;5;241m.\u001b[39marray(trainY)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/indexes/base.py:3636\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3636\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3639\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/indexes/base.py:5651\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5648\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5649\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5650\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5651\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(0, 60, None), slice(0, 54, None))"
     ]
    }
   ],
   "source": [
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 10   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 60  # Number of past days we want to use to predict the future.\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(scaled_x_train) - n_future +1):\n",
    "    trainX.append(scaled_x_train[i - n_past:i, 0:scaled_x_train.shape[1]])\n",
    "    trainY.append(scaled_x_train[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ec9f8c4-95f3-4c27-b140-28569bf1774c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 54)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_values.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3708367a-c480-4a74-981c-2756410c4836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000e047-51f5-4528-89bb-671c52b1589b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "172c84f2-649b-4440-b237-e2a185e372b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 54)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1255e15c-5387-4ec0-b4f8-f36ad1dea19c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 52326 into shape (969,10,54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m n_features \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Reshape the input data to be 3-dimensional\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m test_X_train \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m test_X_test \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mreshape((x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_steps, n_features))\n\u001b[1;32m     10\u001b[0m test_X_train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 52326 into shape (969,10,54)"
     ]
    }
   ],
   "source": [
    "x_train = x_train.values\n",
    "x_test = x_test.values\n",
    "\n",
    "n_features = x_train.shape[1]\n",
    "\n",
    "# Reshape the input data to be 3-dimensional\n",
    "test_X_train = x_train.reshape((x_train.shape[0], n_steps, n_features))\n",
    "test_X_test = x_test.reshape((x_test.shape[0], n_steps, n_features))\n",
    "\n",
    "test_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8e369e1-c9ef-4470-8bd4-5d209071384b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 52326 into shape (969,10,54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m n_features \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Reshape the input data to be 3-dimensional\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m test_X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_X_test \u001b[38;5;241m=\u001b[39m test_values\u001b[38;5;241m.\u001b[39mreshape((test_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_steps, n_features))\n\u001b[1;32m      9\u001b[0m test_x_train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 52326 into shape (969,10,54)"
     ]
    }
   ],
   "source": [
    "# Define the number of time steps and features\n",
    "n_steps = 10\n",
    "n_features = x_train.shape[1]\n",
    "\n",
    "# Reshape the input data to be 3-dimensional\n",
    "test_X_train = train_values.reshape((train_values.shape[0], n_steps, n_features))\n",
    "test_X_test = test_values.reshape((test_values.shape[0], n_steps, n_features))\n",
    "\n",
    "test_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a13a0a35-1bf4-4f03-ae23-66327ffa4837",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(0, 30, None), slice(0, 969, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/_libs/index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(0, 30, None), slice(0, 969, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [88], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#Reformat input data into a shape: (n_samples x timesteps x n_features)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#In my example, my df_for_training_scaled has a shape (12823, 5)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_past, \u001b[38;5;28mlen\u001b[39m(x_train) \u001b[38;5;241m-\u001b[39m n_future \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     x_train_array (\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     13\u001b[0m     y_train_array\u001b[38;5;241m.\u001b[39mappend(y_train[i \u001b[38;5;241m+\u001b[39m n_future \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:i \u001b[38;5;241m+\u001b[39m n_future, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/indexes/base.py:3636\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3633\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3634\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3636\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3639\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/pandas/core/indexes/base.py:5651\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5648\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5649\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5650\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5651\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(0, 30, None), slice(0, 969, None))"
     ]
    }
   ],
   "source": [
    "#Empty lists to be populated using formatted training data\n",
    "x_train_array = []\n",
    "y_train_array = []\n",
    "\n",
    "n_future = 10   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 30  # Number of past days we want to use to predict the future.\n",
    "\n",
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(x_train) - n_future +1):\n",
    "    x_train_array (x_train[i - n_past:i, 0:x_train.shape[0]])\n",
    "    y_train_array.append(y_train[i + n_future - 1:i + n_future, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7ad55-0140-40e6-b511-382a2b2fb7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "50b5070b-993f-43a9-83ef-f686c167b2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 969, 64)           30464     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 969)               31977     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,857\n",
      "Trainable params: 74,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the Autoencoder model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(x_train.shape[0], x_train.shape[1]), return_sequences=True))\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[0]))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "854264d4-58f5-493a-879e-36841b57f7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 969, 54), found shape=(None, 54)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/d1/4c4_qxp904j46j9r74vbnkd80000gn/T/__autograph_generated_file67fo7ncc.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/miniconda3/envs/snowflakes/lib/python3.10/site-packages/keras/engine/input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_5\" is incompatible with the layer: expected shape=(None, 969, 54), found shape=(None, 54)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5, batch_size=16, validation_split=0.1, verbose=1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be3c7d-b9fc-4280-a2f1-4842260bb6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-snowflakes]",
   "language": "python",
   "name": "conda-env-miniconda3-snowflakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
